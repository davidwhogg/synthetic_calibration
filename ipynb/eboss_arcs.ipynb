{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eboss_arcs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhyEIG4AUaLCcdIBfk6aUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/synthetic_calibration/blob/main/ipynb/eboss_arcs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawil0PuWSIO"
      },
      "source": [
        "# Exploratory data analysis with *SDSS-IV eBOSS* arc exposures\n",
        "\n",
        "*by* **David W. Hogg**\n",
        "\n",
        "## Notes:\n",
        "- The most recent public repository of raw *BOSS* spectrograph data is at `\"https://data.sdss.org/sas/dr16/eboss/spectro/data/{:05d}/\".format(mjd)`\n",
        "- The non-public repository of raw *BOSS* spectrograph data is at `\"rsync://sdss@dtn01.sdss.org/sas/ebosswork/eboss/spectro/data/{:05d}/\".format(mjd)`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoXQZDiZuf2N"
      },
      "source": [
        "!pip install corner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnTL9hpXRgeS"
      },
      "source": [
        "import numpy as np\n",
        "import pylab as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from astropy.io import fits\n",
        "import urllib\n",
        "from urllib.request import Request, urlopen, urlretrieve\n",
        "from bs4 import BeautifulSoup\n",
        "import corner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGgYeQ7v1n1A"
      },
      "source": [
        "# STUPID AUTH SHIT\n",
        "if False:\n",
        "  # create a password manager\n",
        "  password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
        "\n",
        "  # Add the username and password.\n",
        "  # If we knew the realm, we could use it instead of None.\n",
        "  top_level_url = \"https://data.sdss.org/sas/ebosswork/\"\n",
        "  password_mgr.add_password(None, top_level_url, \"sdss\", \"censored\") # WRONG PASSWORD\n",
        "  handler = urllib.request.HTTPBasicAuthHandler(password_mgr)\n",
        "\n",
        "  # create \"opener\" (OpenerDirector instance)\n",
        "  opener = urllib.request.build_opener(handler)\n",
        "\n",
        "  # Install the opener.\n",
        "  # Now all calls to urllib.request.urlopen use our opener.\n",
        "  urllib.request.install_opener(opener)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IsDxdqmV4dY"
      },
      "source": [
        "# testing\n",
        "if False:\n",
        "  url = make_boss_data_url(58779)\n",
        "  urls = get_all_sdR_urls(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc2cr0U0RlYH"
      },
      "source": [
        "def make_boss_data_url(mjd):\n",
        "  \"\"\"\n",
        "  Find a public directory of raw boss data, given an MJD.\n",
        "  \"\"\"\n",
        "  # return \"http://localhost:8888/tree/{:05d}/\".format(mjd)\n",
        "  # return \"https://data.sdss.org/sas/ebosswork/eboss/spectro/data/{:05d}/\".format(mjd)\n",
        "  return \"https://data.sdss.org/sas/dr16/eboss/spectro/data/{:05d}/\".format(mjd)\n",
        "\n",
        "def get_all_sdR_urls(mjd, camera=\"r1\"):\n",
        "  \"\"\"\n",
        "  Find all public sdR file urls, given a directory url.\n",
        "\n",
        "  This code is overly specific!\n",
        "  \"\"\"\n",
        "  url = make_boss_data_url(mjd)\n",
        "  req = Request(url)\n",
        "  a = urlopen(req).read()\n",
        "  soup = BeautifulSoup(a, 'html.parser')\n",
        "  x = (soup.find_all('a'))\n",
        "  urls = []\n",
        "  for i in x:\n",
        "    file_name = i.extract().get_text()\n",
        "    if \"sdR-{}-\".format(camera) in file_name:\n",
        "      url_new = url + file_name\n",
        "      # url_new = url_new.replace(\" \",\"%20\")\n",
        "      urls.append(url_new)\n",
        "  return np.unique(urls)\n",
        "\n",
        "def grab_all_arcs(mjd, slitids=(14,), camera=\"r1\"):\n",
        "  \"\"\"\n",
        "  Find all arcs from one slithead on one mjd, from public data.\n",
        "\n",
        "  ## Bugs:\n",
        "  - Terrible >100. hack; I don't know what to do about these zero images?\n",
        "  \"\"\"\n",
        "  arcs = []\n",
        "  try:\n",
        "    sdR_urls = get_all_sdR_urls(mjd, camera=camera)\n",
        "  except:\n",
        "    return arcs\n",
        "  for url in sdR_urls:\n",
        "    hdu = fits.open(url)\n",
        "    hdr = hdu[0].header\n",
        "    if (\"arc\" in hdr[\"FLAVOR\"]) and \\\n",
        "     (hdr[\"SLITID1\"] in slitids):\n",
        "      assert hdr[\"SLITID1\"] == hdr[\"SLITID2\"]\n",
        "      if np.median(hdu[0].data) > 100.: # HACK MAGIC HACK\n",
        "        print(\"found arc\", url)\n",
        "        arcs.append((url, hdr, hdu[0].data))\n",
        "      else:\n",
        "        print(\"bad arc?\", np.median(hdu[0].data), url)\n",
        "        print(hdr)\n",
        "    hdu.close()\n",
        "    del hdu\n",
        "  print(\"On {}, I found a total of {} arcs for slitheads {}.\".format(mjd, len(arcs), slitids))\n",
        "  return arcs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k8XDPrvH-pt"
      },
      "source": [
        "arcs = []\n",
        "for mjd in np.arange(58540, 58542):\n",
        "  arcs += grab_all_arcs(mjd, camera=\"r1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v23eGM_lD3Jb"
      },
      "source": [
        "print(\"total of {} arcs\".format(len(arcs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z964BVxxW2dO"
      },
      "source": [
        "for url, hdr, image in arcs:\n",
        "  print(url, image.shape, hdr[\"ALT\"], hdr[\"AZ\"], hdr[\"IPA\"], hdr[\"EXPTIME\"],\n",
        "        np.min(image), np.median(image), np.max(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XumHQ28kSvFp"
      },
      "source": [
        "with PdfPages('boss_arc_zoom.pdf') as pdf:\n",
        "  for url, hdr, image in arcs:\n",
        "    subimage = image[1700:1900, 1500:1700].astype(float)\n",
        "    subimage -= np.median(subimage)\n",
        "    fig = plt.figure(figsize=(12, 9))\n",
        "    plt.imshow(subimage, vmin=-30, vmax=30.0,\n",
        "               cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.colorbar()\n",
        "    plt.title(url)\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdaZWjhCUCvx"
      },
      "source": [
        "def _cc(dx, dy, image, reference, maxshift):\n",
        "  tmp = np.roll(image, (dy, dx), axis=(0, 1))\n",
        "  numerator = reference * tmp\n",
        "  denominator = tmp * tmp\n",
        "  return np.sum(numerator[maxshift:-maxshift,maxshift:-maxshift]) # / \\\n",
        "#       np.sum(denominator[maxshift:-maxshift,maxshift:-maxshift])\n",
        "\n",
        "def cross_correlate(image, reference, maxshift=8):\n",
        "  assert image.shape == reference.shape\n",
        "  shifts = np.arange(-maxshift, maxshift + 1)\n",
        "  ccf = np.zeros((len(shifts), len(shifts)))\n",
        "  for ix, dx in enumerate(shifts):\n",
        "    for iy, dy in enumerate(shifts):\n",
        "      ccf[iy, ix] = _cc(dx, dy, image, reference, maxshift)\n",
        "  return ccf, shifts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKCv3ygpd43H"
      },
      "source": [
        "def imslice(image):\n",
        "  return (image[1700:1900, 1500:1700]).astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeWywC1OhDhF"
      },
      "source": [
        "def centroid_3(patch, xx=np.arange(-1, 2)):\n",
        "  assert patch.shape == (3, )\n",
        "  assert xx.shape == (3, )\n",
        "  features = [xx ** 0,\n",
        "              xx, \n",
        "              0.5 * xx ** 2]\n",
        "  MT = np.vstack(features)\n",
        "  MTM = MT @ MT.T\n",
        "  pars = np.linalg.lstsq(MTM, MT @ patch.flatten(), rcond=None)[0]\n",
        "  center = -1.0 * pars[1] / pars[2]\n",
        "  assert center > np.min(xx)\n",
        "  assert center < np.max(xx)\n",
        "  return center\n",
        "\n",
        "def centroid_max(image):\n",
        "  assert len(image.shape) == 2\n",
        "  ismax = np.where(image == np.max(image))\n",
        "  besty, bestx = ismax[0][0], ismax[1][0]\n",
        "  bestpatch = image[besty-1:besty+2, bestx-1:bestx+2]\n",
        "  plt.imshow(bestpatch, interpolation=\"nearest\", cmap=\"gray\")\n",
        "  ymax = besty + centroid_3(np.sum(bestpatch, axis=1))\n",
        "  xmax = bestx + centroid_3(np.sum(bestpatch, axis=0))\n",
        "  return (ymax, xmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om_AfOrX5Tjl"
      },
      "source": [
        "# make rectangular data for plotting (and plot along the way)\n",
        "refimage = imslice(arcs[10][2]) # magic 10\n",
        "refimage -= np.median(refimage)\n",
        "plotdata = np.zeros((len(arcs), 4))\n",
        "with PdfPages('boss_arc_ccf.pdf') as pdf:\n",
        "  for i, (url, hdr, image) in enumerate(arcs):\n",
        "    subimage = imslice(image)\n",
        "    subimage -= np.median(subimage)\n",
        "    ccf, shifts = cross_correlate(subimage, refimage)\n",
        "    ymax, xmax = centroid_max(ccf)\n",
        "    xshift = np.interp(xmax, np.arange(len(shifts)), shifts)\n",
        "    yshift = np.interp(ymax, np.arange(len(shifts)), shifts)\n",
        "    fig = plt.figure(figsize=(12, 9))\n",
        "    plt.imshow(ccf,\n",
        "               cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.colorbar()\n",
        "    plt.title(url + \" ({},{})\".format(yshift, xshift))\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "    plotdata[i] = hdr[\"ALT\"], hdr[\"AZ\"], hdr[\"IPA\"], yshift\n",
        "labels = [\"altitude (deg)\", \"azimuth (deg)\", \"position angle (deg)\", \"dispersion shift (pix)\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz6J6pqjqbq6"
      },
      "source": [
        "# make rectangular data for corner plotting\n",
        "f = corner.corner(plotdata, labels=labels, plot_contours=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wWfRtgZv9vy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}