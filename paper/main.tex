% This document is part of the synthetic_calibration project. Copyright 2021 the author(s).

% style notes
% -----------
% - Everything is an image; an image taken at the telescope is data or a data image; a code-generated image is a synthesized image.
% - Data is a mass noun (like grass).

\documentclass[modern]{aastex631}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

% page layout
\renewcommand{\twocolumngrid}{}
\setlength{\parindent}{1.5em}
\raggedbottom\sloppy\sloppypar\frenchspacing
\shorttitle{synthetic calibration for spectroscopy}
\shortauthors{hogg}

% text macros
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\SDSSV}{\project{\acronym{SDSS-V}}}
\newcommand{\BOSS}{\project{\acronym{BOSS}}}
\newcommand{\APOGEE}{\project{\acronym{APOGEE}}}

\begin{document}

\title{Synthetic calibration for spectroscopy: Reducing arc and flat overheads for time-domain projects}
\author{David~W.~Hogg}
\affil{Flatiron Institute, a division of the Simons Foundation}
\date{July 2021}

\begin{abstract}\noindent
    It is likely that the current plans for \SDSSV{} include too many arc and flat exposures; any reduction in calibration overheads translates directly into survey speed.
    Here I propose a method to establish---using only existing \BOSS{} and \APOGEE{} data---whether the future survey could get by with far less overhead than the current plans of having of order one arc and one flat per telescope pointing.
    The method involves a regression, in which we learn a model for the (full, two-dimensional) calibration data as a function of housekeeping data about time, temperature, and telescope orientation or state.
    It is possible that the synthetic calibration data generated by the regression will be, in some cases, more precise and more accurate than real calibration data; after all, they will have been learned from thousands of training images.
\end{abstract}

\section{Introduction}\label{sec:intro}\noindent
Spectrographs vary in their properties as they observe.
For this reason, we have to calibrate them, using (at least) flats and arcs (or laser-frequency combs or etalons or equivalent).
However, no spectrograph varies \emph{arbitrarily}, so it ought to be possible to build a low-dimensional model for the variation of the device, and use the calibration data to locate the device within the (very limited) space of all possible calibration variations.
If that's possible, it opens up the possibility that calibration overheads could be lessened, because---in some sense---when the low-dimensional space of calibration states is understood, the project needs to learn less from any individual calibration exposure or datum.

In prior work (\citealt{excalibur}), we have shown that information from many calibration exposures (specifically laser-frequency comb exposures illuminating a precision radial-velocity spectrograph) can be combined into a very precise, accurate, and practical model for the variability of the calibration state of the spectrograph.
Although that prior work is on a highly stabilized spectrograph, we believe that the general principles will be applicable to any spectrograph, in some form.
Here we ask whether it could be applied to the \BOSS{} spectrographs hanging off the Apache Point Observatory and Las Campa\~nas Observatory 2.5-m telescopes.
Our goal here is not to deliver a working pipeline.
Our goal here is to find out, empirically, whether this might be possible, using the data we have in hand.

\section{Assumptions}\label{sec:ass}\noindent
If we are going to build some tests, I think it helps to write down what we are testing. Perhaps it is the following:
\begin{description}
    \item[Low-dimensional space]
    We assume that the spectrograph can only vary in a finite number of directions, or that variations in the spectrograph calibration state can be described by smooth variations of a small number (like a half-dozen-ish) latent variables.
    The space might not be a precisely \emph{linear} function of latent variables, but the number of latent variables, or the dimensionality of the space, will be small.
    \item[Sufficient housekeeping data]
    We assume that the spectrograph variability or changes in the calibration state can be explained mainly or completely in terms of known housekeeping data, like the altitude, azimuth, field rotation of the telescope, the cartridge or slithead ID, the ambient temperature (and possibly other weather data), the temperatures of internal spectrograph components, the time of night, the time of year, and other known physical parameters.
    If there are calibration variabilities that are \emph{not} predicted well by the known housekeeping data, we hope or assume that we can make new measurements of things (like perhaps the locations of the spectral traces and the like) that can be used to augment or complete the necessary housekeeping data.
\end{description}
If these assumptions hold---approximately or precisely---then it should be possible to train a data-driven or empirical model that predicts calibration data as a function of housekeeping data.
That is, we can build a (possibly nonlinear) regression of calibration images (arcs especially, but also flats and even zeros) against the housekeeping data.
This regression model can then be used to predict or synthesize calibration images for new observations for which there are not calibration data taken, or for which the calibration data were withheld in training.

\section{Notation and plan}\label{sec:plan}\noindent
Fundamentally, the plan is to train the regression described in \secref{sec:ass} on a training set, and test that regression on a validation set.
One nice thing is that, if this all works, any synthesized calibration image will be \emph{better} than any real calibration image, since it will have been constructed using \emph{very large numbers} of calibration images.
Another nice thing is that it will be possible to assess the quality of the regression entirely empirically; it will not involve theoretical beliefs about the spectrographs (or none beyond the assumptions given in \secref{sec:ass}).
Yet another nice thing is that if we succeed, this regression model will be a calibration-data \emph{generator} that can make calibration data for science exposures for which we have no such data (or bad data).

There are several kinds of calibration data.
Let's start with arcs, say.
Everything will apply to flats and darks and all the other things we take (are there other things?).
We start by defining a \emph{training set} of $N$ arc images, and a \emph{validation set} of $M$ held-out arc images, with no overlap of these two sets (they are disjoint).
In principle we can also do a $K$-fold cross-validation setup in which there are $K$ splits of the data into training and validation, such that each arc image is in exactly one of the $K$ validation sets, and exactly $K-1$ of the $K$ training sets.

Each arc image $y_n$ in the training set is an $N_x\times N_y$ image taken at time $t_n$, with $1\leq n\leq N$.
Associated with each arc image $y_n$ we have a vector (or blob, but let's say vector) of housekeeping information $x_n$ that records the state of the telescope and the world at the time of that observation.
In the validation set, we have images $y_m$ taken at times $t_m$ with housekeeping data $x_m$ and $1\leq m\leq M$.
Our goal is to learn the parameters $\theta$ (yes a very very very large number of parameters) of a function $f()$ that predicts the images from the housekeeping data:
\begin{align}
    y_n &= f(x_n; \theta) + \text{noise}
    ~,
\end{align}
where I have summarized all the deviations between the observed image $y_n$ and the predicted image $f(x_n;\theta)$ with the word ``noise''.
Incredibly briefly, the idea is that
\begin{align}\label{eq:loss}
    \hat{\theta} &\leftarrow \arg\min_\theta\left[\sum_{n=1}^N L\!\left(y_n - f(x_n;\theta)\right)\right]
    \\
    L\!\left(\Delta\right) &= \frac{1}{2}\,\Delta^T\,C^{-1}\,\Delta
    ~,
\end{align}
where the best-fit parameters $\hat{\theta}$ are obtained by optimization of a loss function,
and the loss function $L(\Delta)$ is a quadratic form of the residual $\Delta$ with some metric $C^{-1}$.
In \eqref{eq:loss}, we have implicitly reformatted the images $y_n$ and their predictions $f(x_n;\theta)$ (and the residual $\Delta$) into enormous column vectors of length $N_x\,N_y$.

Issues I anticipate include the following:
\begin{itemize}
    \item The calibration images will shift and warp with housekeeping data.
    That is, the variations of the calibration data will not obviously look like linear components, adding and subtracting in image space.
    \emph{I love linear regression.} However, it might not be sufficient for this problem.
    One thought for how to deal with this is to go FullMachineLearning(tm).
    Another thought is to put the images into the Fourier domain, where warping and shifting changes might look more linear or simple.
    I'm confused on this point, because my intuition says Fourier is easier, but I also know that the Fourier domain doesn't change linearity, so what gives in my intuition?
    Bill Freeman (MIT) teaches us that going Fourier (or wavelet) can regularize regressions. But my intuition is wrong.
    \item
    I'm guessing that we don't have all of the housekeeping data we would ideally like to have.
    \item
    It is likely that there are variations in the calibration data that go beyond the housekeeping data.
    For example, do we know the arc-lamp temperatures?
    That is, there might need to be additional latent parameters to learn along with the images.
    These parameters might need to be learned with the science data to which the calibration frames are going to be applied.
    If this is true, the structure of the model will need to be changed, the model will become a kind of self-calibration, and it will be harder to implement.
    \item
    There are probably instrument changes and adjustments such that the calibration data are not continuously varying with housekeeping data, but have jumps at certain times.
    This could be determined empirically or theoretically from the instrument maintenance schedules.
    But really we probably need a model for each (what you might call) ``instrument epoch'', for a few different finite-time epochs.
\end{itemize}

\section{Discussion and proposal}\label{sec:proposal}\noindent

\begin{thebibliography}{}
\bibitem[Zhao et al.(2021)]{excalibur} Zhao, L.~L., Hogg, D.~W., Bedell, M., et al.\ 2021, \aj, 161, 80. doi:10.3847/1538-3881/abd105
\end{thebibliography}

\end{document}
